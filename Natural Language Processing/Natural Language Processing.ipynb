{"cells":[{"cell_type":"markdown","metadata":{"id":"VwK5-9FIB-lu","colab_type":"text"},"source":["# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"X1kiO9kACE6s","colab_type":"text"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","import pandas as pd "]},{"cell_type":"markdown","metadata":{"id":"wTfaCIzdCLPA","colab_type":"text"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# delimiter is the seperator of data file\n","# we will ignore quotes using quoting\n","dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = \"\\t\", quoting = 3)\n"]},{"cell_type":"markdown","metadata":{"id":"Qekztq71CixT","colab_type":"text"},"source":["## Cleaning the texts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocessing and cleaning texts\n","import re                       # regular expression library\n","import nltk                     # text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers\n","\n","nltk.download('stopwords')      # Download all stopping words such as 'a', 'and', 'the', etc that are articles which doesnt give hint about review\n","\n","from nltk.corpus import stopwords \n","from nltk.stem.porter import PorterStemmer      # Remove tenses from words such as 'loved' and 'love'\n","\n","# Cleanig the text\n","corpus = []                 # all the cleaned texts\n","for i in range(0, 1000):\n","    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])                           # using Regular expression to remove punctuations\n","    review = review.lower()                                                           # lower all characters\n","\n","    review = review.split()                                                           # splitting review into an array of words\n","\n","    ps = PorterStemmer()\n","\n","    allStopWords = stopwords.words('english')\n","    allStopWords.remove('not')\n","\n","    review = [ps.stem(word) for word in review if not word in set()]      # stemming each word from review list while neglecting the stopword\n","    review = ' '.join(review)   # join words of list into one element\n","\n","    corpus.append(review)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend"]},"outputs":[],"source":["# print(corpus)"]},{"cell_type":"markdown","metadata":{"id":"CLqmAkANCp1-","colab_type":"text"},"source":["## Creating the Bag of Words model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenication using sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)                       # neglect the uncommon words such as names, that only apear once, etc\n","\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:, -1].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# len(X[0])"]},{"cell_type":"markdown","metadata":{"id":"DH_VjgPzC2cd","colab_type":"text"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)"]},{"cell_type":"markdown","metadata":{"id":"VkIq23vEDIPt","colab_type":"text"},"source":["## Training the Naive Bayes model on the Training set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# any classification model can be used\n","from sklearn.naive_bayes import GaussianNB\n","\n","classifier = GaussianNB()\n","classifier.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"1JaRM7zXDWUy","colab_type":"text"},"source":["## Predicting the Test set results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = classifier.predict(X_test)\n","print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"]},{"cell_type":"markdown","metadata":{"id":"xoMltea5Dir1","colab_type":"text"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","\n","print(cm)\n","accuracy_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# accuract can be imporved by more data and not excluding words that does affect review"]}],"metadata":{"colab":{"name":"Natural Language Processing","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM9tx3HllsdwqqTLZQw/zx5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}